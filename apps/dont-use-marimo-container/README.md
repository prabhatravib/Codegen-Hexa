# Marimo Container for CodeGen Hexa

This container provides a working Marimo notebook environment for rendering LLM-generated notebooks from the main CodeGen Hexa application.

## ğŸš€ Features

- **Cloudflare Containers**: Deployed using Cloudflare Workers with container support
- **LLM Integration**: Receives and renders notebooks generated by AI from the main backend
- **Working Rendering**: Uses the proven approach from `twilight-cell-b373` for reliable notebook display
- **UUID-based Notebooks**: Each notebook gets a unique identifier for isolation
- **Real-time Updates**: Full Marimo functionality with WebSocket support

## ğŸ”§ How It Works

### 1. Notebook Generation Flow
```
User Input (Mermaid Chart) â†’ Backend LLM â†’ Generate Marimo Notebook â†’ Save to Container â†’ Render
```

### 2. Container Operation
- Container starts with a fallback template notebook
- Receives LLM-generated notebooks via `/create-notebook` endpoint
- Creates notebook files in `/app/notebooks` directory
- Marimo server renders notebooks with full interactivity

### 3. Integration Points
- **Backend**: `apps/backend/src/routes/marimo.ts` - Generates notebooks and calls container
- **Container**: `apps/marimo-container/` - Renders notebooks using working approach
- **Frontend**: Receives container URLs for notebook display

## ğŸ“ Project Structure

```
apps/marimo-container/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ index.ts                 # Cloudflare Worker entry point
â”‚   â”œâ”€â”€ start_marimo.py          # Container startup script
â”‚   â”œâ”€â”€ create_uuid_notebook.py  # Fallback notebook creation
â”‚   â””â”€â”€ create_llm_notebook.py   # LLM notebook creation
â”œâ”€â”€ Dockerfile                   # Container definition
â”œâ”€â”€ wrangler.toml               # Cloudflare Workers configuration
â”œâ”€â”€ requirements.txt             # Python dependencies
â””â”€â”€ test_container.py           # Local testing script
```

## ğŸš€ Deployment

### Deploy to Cloudflare
```bash
cd apps/marimo-container
wrangler deploy
```

### Local Testing
```bash
# Test the container locally
python test_container.py

# Build and test Docker container
docker build -t marimo-container .
docker run -p 2718:2718 marimo-container
```

## ğŸ”Œ API Endpoints

### Container Endpoints
- `POST /create-notebook` - Create notebook from LLM content
- `GET /{notebook_name}` - Access rendered notebook
- All other requests forwarded to Marimo server

### Backend Integration
- `POST /api/marimo/save-to-container` - Save LLM notebook to container
- Returns container URL for frontend display

## âš™ï¸ Configuration

### Environment Variables
- `MARIMO_SKIP_UPDATE_CHECK=1` - Skip Marimo update checks
- `MARIMO_LOG_LEVEL=INFO` - Set logging level

### Container Settings
- **Port**: 2718 (Marimo default)
- **Max Instances**: 10
- **Sleep After**: 2 hours of inactivity

## ğŸ” Troubleshooting

### Common Issues
1. **Container Not Starting**: Check Docker build and requirements
2. **Notebook Not Rendering**: Verify Marimo server is running on port 2718
3. **LLM Integration Failing**: Check backend endpoint and container communication

### Debug Mode
- Container logs show detailed startup information
- Test endpoints available for verification
- Health check at `/health` endpoint

## ğŸ¯ Benefits

1. **Working Rendering**: Replaces broken notebook mounting with proven approach
2. **LLM Integration**: Maintains AI-generated notebook workflow
3. **Cloudflare Native**: Optimized for Cloudflare Workers deployment
4. **Scalable**: Multiple container instances for load handling
5. **Isolated**: Each notebook session is completely separate

## ğŸš€ Next Steps

Consider these enhancements:
- Notebook persistence in Cloudflare KV
- Multiple notebook templates
- User preference storage
- Collaborative features
- Custom domain support

---

**Note**: This implementation integrates the working Marimo approach from `twilight-cell-b373` with the existing LLM-based notebook generation workflow.
